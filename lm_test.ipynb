{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "## Importing Libraries ##\n",
    "#########################\n",
    "\n",
    "\n",
    "# Numerical Libraries\n",
    "import numpy as np\n",
    "from jax import random, jit, vmap, grad, jacobian, hessian, lax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "jnp_float = jnp.float32\n",
    "\n",
    "# Tools from JAX\n",
    "from jax.numpy.fft import fft, ifft\n",
    "from jaxopt import BFGS\n",
    "\n",
    "# Data Libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# Plotting Libraries\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Timing/Progress Libraries\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# System Libraries\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## RNG SETUP ##\n",
    "###############\n",
    "\n",
    "# initialize random key\n",
    "rng, _ = random.split(random.PRNGKey(12345))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Signal Creation Functions ##\n",
    "###############################\n",
    "\n",
    "# decay function produces an exponential decay signal\n",
    "@jit            # This is a decorator that compiles the function to make it faster\n",
    "def decay(A, ùúè, t):\n",
    "    return A * jnp.exp(-t/ùúè)\n",
    "\n",
    "# multi_decay function produces a sum of exponential decay signals \n",
    "@jit\n",
    "def multi_decay(params, t):\n",
    "    \n",
    "    # first lets check that the number of parameters is even and at least 2\n",
    "    assert len(params) >= 2, \"Number of parameters must be at least 2\"\n",
    "    assert len(params) % 2 == 0, \"Number of parameters must be even\"\n",
    "    \n",
    "    A = jnp.array(params[::2])\n",
    "    ùúè = jnp.array(params[1::2])\n",
    "    \n",
    "    signal = jnp.zeros_like(t)\n",
    "    for i in range(len(A)):\n",
    "        signal += decay(A[i], ùúè[i], t)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "def mean_bootstrap(key, signal):\n",
    "    sample_indices = random.randint(\n",
    "        key=key, \n",
    "        shape=(signal.shape[0],), \n",
    "        minval=0, \n",
    "        maxval=signal.shape[0]\n",
    "    )\n",
    "    signal_sample = signal[sample_indices]\n",
    "    return jnp.mean(signal_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## 1D Signal Creation ##\n",
    "########################\n",
    "\n",
    "# initialize random key for signal creation\n",
    "# this guarantees that the signal is the same every time the code is run\n",
    "rng, _ = random.split(random.PRNGKey(12345))\n",
    "\n",
    "# Create the time array\n",
    "n = 2726\n",
    "dt = 480 # microseconds used to avoid floating point errors in t\n",
    "t_max = n * dt\n",
    "t = jnp.linspace(0, t_max, n, endpoint=False)/1000 # convert to milliseconds\n",
    "\n",
    "# Create the signal\n",
    "params = jnp.array([\n",
    "    1.0, 100,\n",
    "])\n",
    "signal = multi_decay(params, t)\n",
    "\n",
    "# Add noise to the signal\n",
    "SNR = 100\n",
    "noise_amplitude = jnp.sum(params[::2]) / SNR\n",
    "rng, key = random.split(rng)\n",
    "noisy_signal = signal + noise_amplitude * random.normal(key, signal.shape)\n",
    "\n",
    "# Find the true mean of the noisy signal for use in true_params\n",
    "d_true = jnp.mean(signal) + jnp.mean(noise_amplitude * random.normal(key, signal.shape))\n",
    "\n",
    "# Alter signal with random offset\n",
    "rng, key = random.split(rng)\n",
    "noisy_signal = noisy_signal + random.uniform(key)\n",
    "\n",
    "# bootstrap the mean of the noisy signal\n",
    "keys = random.split(rng, 5001)\n",
    "rng, keys = keys[0], keys[1:]\n",
    "\n",
    "mean_bootstrap_jit = jit(lambda key: mean_bootstrap(key, noisy_signal))\n",
    "\n",
    "# generate 1000 bootstrapped means\n",
    "means = vmap(mean_bootstrap_jit)(keys)\n",
    "\n",
    "# find the mean and standard deviation of the bootstrapped means\n",
    "d_mean = jnp.mean(means)\n",
    "d_variance = jnp.var(means)\n",
    "\n",
    "# Adjust noisy signal for mean \n",
    "noisy_signal = noisy_signal - d_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Functions Needed For Fitting ##\n",
    "##################################\n",
    "\n",
    "@jit\n",
    "def linear_solve(noisy_signal, t, tau):\n",
    "    \"\"\"\n",
    "    This function solves for the best A0 and c given a noisy signal and a tau value.\n",
    "    \n",
    "    Args:\n",
    "        noisy_signal: The signal to be fit to\n",
    "        t: The time array that the signal is sampled at\n",
    "        tau: The decay constant(s) we are fitting for\n",
    "    \n",
    "    \n",
    "    The core idea is that if our signal takes the form\n",
    "    \n",
    "    f(t) = A_1 * e^{-t/tau_1} + A_2 * e^{-t/tau_2} + c,\n",
    "    \n",
    "    for example, then we can rewrite this a linear system of equations\n",
    "    \n",
    "    f(t) = A_1 * phi_1(t) + A_2 * phi_2(t) + c * phi_3(t),\n",
    "    \n",
    "    where phi_1(t) = e^{-t/tau_1}, phi_2(t) = e^{-t/tau_2}, and phi_3(t) = 1.\n",
    "    There is a single solution to this system of equations given by\n",
    "    \n",
    "    [A_1, A_2, c] = (Phi^T Phi)^{-1} Phi^T f(t),\n",
    "    \n",
    "    where Phi is the matrix with columns phi_1, phi_2, and phi_3. The weirdness with the \n",
    "    (Phi^T Phi)^{-1} term comes from the fact that phi is not square and so we can't invert it directly.\n",
    "    \n",
    "    \n",
    "    NOTE: This function groups all amplitude phis together in phi_1 and the constant phi in phi_2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the Amplitude phi\n",
    "    phi_1 = jnp.exp(-t[:, None]/tau[None, :])\n",
    "    # Construct the constant phi\n",
    "    phi_2 = jnp.ones_like(t)[:, None]\n",
    "    \n",
    "    # Stack the phis together\n",
    "    Phi = jnp.column_stack([phi_1, phi_2])\n",
    "    \n",
    "    \n",
    "    inv = jnp.linalg.inv(Phi.T @ Phi)\n",
    "    \n",
    "    params = (inv @ Phi.T @ noisy_signal[:, None]).flatten()\n",
    "    \n",
    "    return params\n",
    "\n",
    "def create_loss_function(t, noisy_signal):\n",
    "    @jit\n",
    "    def loss_function(tau):\n",
    "        linear_params = linear_solve(noisy_signal, t, tau)\n",
    "        A_vals, c_val = linear_params[:-1], linear_params[-1]\n",
    "\n",
    "        fit = jnp.sum(A_vals[:, None] * jnp.exp(-t[None, :]/tau[:, None]), axis=0) + c_val\n",
    "\n",
    "        return jnp.sqrt(jnp.mean((noisy_signal - fit)**2))\n",
    "    \n",
    "    return loss_function\n",
    "\n",
    "loss_function = create_loss_function(t, noisy_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "## Jit Wrapped Levenberg‚ÄìMarquardt Algorithm ##\n",
    "###############################################\n",
    "\n",
    "def create_lm_optimize(\n",
    "    dim,\n",
    "    loss_function, \n",
    "    max_steps=200, \n",
    "    base_lr=1.0, \n",
    "    initial_damping=1e-5, \n",
    "    damping_scale=4.0,\n",
    "    tol=1e-12,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    This is an implementation of a single step of the Levenberg-Marquardt algorithm that \n",
    "    doesn't use the Gauss-Newton approximation. This approximation is useful but in our use \n",
    "    case with a long signal and few parameters, it is very expensive in comparison.    \n",
    "    \n",
    "    The traditional Newton's method uses this update step: \n",
    "    \n",
    "    theta = theta - lr * inv(|Hess|) @ grad   (multiple parameters)\n",
    "    theta = theta - lr * grad / |Hess|   (One parameter)\n",
    "    \n",
    "    Whereas gradient descent uses this update step:\n",
    "    \n",
    "    theta = theta - lr * grad \n",
    "    \n",
    "    This is equivalent to replacing the hessian with an identity matrix.\n",
    "    \n",
    "    The Levenberg-Marquardt algorithm combines the two by using a weighting factor applied to both:\n",
    "    \n",
    "    theta = theta - lr * inv(|(1-damping) * Hess + damping * I|) @ grad  (multiple parameters)\n",
    "    theta = theta - lr * grad / |(1-damping) * Hess + damping|  (One parameter)\n",
    "    \n",
    "    The damping factor allows the algorithm to switch between gradient descent and newton's method\n",
    "    depending on which is working with the current curvature of the loss landscape.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    grad_fn = jit(grad(loss_function))\n",
    "    hess_fn = jit(hessian(loss_function))\n",
    "\n",
    "    @jit\n",
    "    def apply_param_constraints(params):\n",
    "        params = jnp.where(params < 0, jnp.abs(params), params)\n",
    "        return jnp.where(params == 0, 1.0, params)\n",
    "    \n",
    "    @jit\n",
    "    def lm_step(params, prev_loss, lr, damping):\n",
    "        \n",
    "        # calculate the gradient and hessian\n",
    "        g = grad_fn(params)\n",
    "        H = hess_fn(params)\n",
    "        \n",
    "        # Handle scalar and vector parameters\n",
    "        if dim == 1:\n",
    "            damped_H = jnp.abs((1 - damping) * H + damping)\n",
    "            step = lr * g / damped_H[0]\n",
    "        else:\n",
    "            damped_H = jnp.abs((1 - damping) * H + damping * jnp.eye(params.size))\n",
    "            step = lr * jnp.linalg.solve(damped_H, g)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = params - step\n",
    "        new_params = apply_param_constraints(new_params)\n",
    "        \n",
    "        new_loss = loss_function(new_params)\n",
    "        \n",
    "        # Calculate rho for use in adaptive damping\n",
    "        actual_reduction = prev_loss - new_loss\n",
    "        if dim == 1:\n",
    "            predicted_reduction = scalar_reduction(g, H[0], step)[0]\n",
    "        else:\n",
    "            predicted_reduction = vector_reduction(g, H, step)\n",
    "        \n",
    "        rho = actual_reduction / jnp.abs(predicted_reduction)\n",
    "        \n",
    "        done = jnp.logical_and(actual_reduction == 0, jnp.abs(predicted_reduction) < tol)\n",
    "        \n",
    "        return new_params, new_loss, rho, done\n",
    "    \n",
    "    @jit\n",
    "    def scalar_reduction(prev_g, prev_H, step):\n",
    "        return 0.5 * (prev_g * step - prev_H * step**2)\n",
    "    \n",
    "    @jit\n",
    "    def vector_reduction(prev_g, prev_H, step):\n",
    "        return 0.5 * (jnp.dot(prev_g, step) - 0.5 * jnp.dot(step, jnp.dot(prev_H, step)))\n",
    "    \n",
    "    @jit\n",
    "    def adaptive_damping_update(tau, loss, prev_tau, prev_loss, rho, damping):\n",
    "        \n",
    "        # First condition: Handle high rho values (successful step)\n",
    "        damping = lax.cond(\n",
    "            rho > 1.0, # if rho is greater than 1\n",
    "            lambda x: x / damping_scale, # Decrease damping\n",
    "            lambda x: x, # Otherwise keep current damping\n",
    "            damping\n",
    "        )\n",
    "        \n",
    "        # Second condition: Handle moderate rho values (partially successful step)\n",
    "        damping = lax.cond(\n",
    "            jnp.logical_and(rho >= 0, rho < 0.5), # if rho is between 0 and 0.5\n",
    "            lambda x: jnp.minimum(x * damping_scale, 1.0), # Increase damping\n",
    "            lambda x: x, # Otherwise keep current damping\n",
    "            damping\n",
    "        )\n",
    "        \n",
    "        # Third condition: Handle negative rho (failed step)\n",
    "        operand = (tau, loss, damping, prev_tau, prev_loss)\n",
    "        tau, loss, damping = lax.cond(\n",
    "            rho < 0, # if rho is negative\n",
    "            lambda op: (op[3], op[4], jnp.minimum(op[2] * damping_scale**2, 1.0)),  # Revert to previous\n",
    "            lambda op: (op[0], op[1], op[2]),  # Otherwise keep current damping\n",
    "            operand\n",
    "        )\n",
    "        \n",
    "        # Implicit fourth condition: If rho is between 0.5 and 1.0, do nothing\n",
    "        \n",
    "        # Return the updated values\n",
    "        return tau, loss, damping\n",
    "    \n",
    "    \n",
    "    def lm_optimize(\n",
    "        initial_params,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Levenberg-Marquardt optimization using JAX for automatic differentiation and JIT compilation.\n",
    "\n",
    "        Args:\n",
    "            initial_params: Initial parameter values (scalar or array).\n",
    "            loss_function: Function to minimize. Must be compatible with JAX grad/hessian.\n",
    "            max_steps: Maximum number of optimization steps (default: 200).\n",
    "            base_learning_rate: Minimum learning rate (default: 1.0).\n",
    "            initial_damping: Initial damping factor (default: 1e-5).\n",
    "            damping_rate: Factor to adjust damping (default: 4.0).\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing optimization results and metrics.\n",
    "        \"\"\"\n",
    "        \n",
    "        if initial_params.ndim == 0:\n",
    "            assert dim == 1, \"Scalar parameters require a dimension of 1. Either use a vector or change the dimension to 1.\"\n",
    "        else:\n",
    "            # make sure that parameters is a 1d array\n",
    "            assert initial_params.ndim == 1, \"Parameters must be a 1D array.\"\n",
    "            assert dim == initial_params.size, \"Vector parameters require a dimension equal to the number of parameters.\"\n",
    "        \n",
    "        print(f\"optimizer has been called with initial_params: {initial_params}\")\n",
    "        \n",
    "        tau = initial_params\n",
    "        loss = loss_function(tau)\n",
    "        damping = initial_damping\n",
    "        lr = base_lr\n",
    "        \n",
    "        lm_time = 0.0\n",
    "        adu_time = 0.0\n",
    "        \n",
    "        print(f\"Initial Setup Complete.\")\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            start = time()\n",
    "            tau_new, loss_new, rho, done = lm_step(tau, loss, lr, damping)\n",
    "            lm_time += time() - start\n",
    "            \n",
    "            # Exit condition, both the actual and predicted reductions are zero\n",
    "            if done:\n",
    "                tau = tau_new\n",
    "                loss = loss_new\n",
    "                break\n",
    "            \n",
    "            start = time()\n",
    "            tau, loss, damping = adaptive_damping_update(tau_new, loss_new, tau, loss, rho, damping)\n",
    "            adu_time += time() - start\n",
    "            \n",
    "            print(f\"Step: {i+1}, Loss: {loss}, Tau: {tau}, Damping: {damping}\")\n",
    "            \n",
    "        \n",
    "        print(f\"LM Time: {lm_time}\")\n",
    "        print(f\"ADU Time: {adu_time}\")\n",
    "        \n",
    "        return tau, loss, i+1\n",
    "    \n",
    "    return lm_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer has been called with initial_params: [50.]\n",
      "Initial Setup Complete.\n",
      "Step: 1, Loss: 0.009999190457165241, Tau: [98.62356], Damping: 2.499999936844688e-06\n",
      "Step: 2, Loss: 0.009903169237077236, Tau: [100.11884], Damping: 6.24999984211172e-07\n",
      "Step: 3, Loss: 0.009903053753077984, Tau: [100.17234], Damping: 1.56249996052793e-07\n",
      "LM Time: 1.1589608192443848\n",
      "ADU Time: 0.023734569549560547\n",
      "optimizer has been called with initial_params: [200.]\n",
      "Initial Setup Complete.\n",
      "Step: 1, Loss: 0.03642088174819946, Tau: [150.15526], Damping: 2.499999936844688e-06\n",
      "Step: 2, Loss: 0.03642088174819946, Tau: [150.15526], Damping: 3.9999998989515007e-05\n",
      "Step: 3, Loss: 0.028130440041422844, Tau: [135.38652], Damping: 9.999999747378752e-06\n",
      "Step: 4, Loss: 0.028130440041422844, Tau: [135.38652], Damping: 0.00015999999595806003\n",
      "Step: 5, Loss: 0.025882577523589134, Tau: [131.61632], Damping: 3.9999998989515007e-05\n",
      "Step: 6, Loss: 0.01587296836078167, Tau: [115.28142], Damping: 9.999999747378752e-06\n",
      "Step: 7, Loss: 0.01587296836078167, Tau: [115.28142], Damping: 0.00015999999595806003\n",
      "Step: 8, Loss: 0.013842006213963032, Tau: [111.75033], Damping: 3.9999998989515007e-05\n",
      "Step: 9, Loss: 0.010058069601655006, Tau: [102.17964], Damping: 9.999999747378752e-06\n",
      "Step: 10, Loss: 0.00990375503897667, Tau: [100.30592], Damping: 2.499999936844688e-06\n",
      "Step: 11, Loss: 0.009903054684400558, Tau: [100.17661], Damping: 6.24999984211172e-07\n",
      "Step: 12, Loss: 0.009903053753077984, Tau: [100.17284], Damping: 1.56249996052793e-07\n",
      "LM Time: 0.0\n",
      "ADU Time: 0.0\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "## Testing LM Function on 1D Signal ##\n",
    "######################################\n",
    "\n",
    "lm_optimize = create_lm_optimize(\n",
    "    dim=1,\n",
    "    loss_function=create_loss_function(t, noisy_signal),\n",
    "    max_steps=100,\n",
    "    base_lr=1.0,\n",
    "    initial_damping=1e-5,\n",
    "    damping_scale=4.0,\n",
    "    tol=1e-12,\n",
    ")\n",
    "\n",
    "tau, loss, steps = lm_optimize(jnp.array([50.0]))\n",
    "\n",
    "tau, loss, steps = lm_optimize(jnp.array([200.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## 2D Signal Creation ##\n",
    "########################\n",
    "\n",
    "# initialize random key for signal creation\n",
    "# this guarantees that the signal is the same every time the code is run\n",
    "rng, _ = random.split(random.PRNGKey(1234567))\n",
    "\n",
    "# Create the signal\n",
    "params = jnp.array([\n",
    "    0.6, 40.0,\n",
    "    0.4, 200.0\n",
    "])\n",
    "signal = multi_decay(params, t)\n",
    "\n",
    "# Add noise to the signal\n",
    "SNR = 100\n",
    "noise_amplitude = jnp.sum(params[::2]) / SNR\n",
    "rng, key = random.split(rng)\n",
    "noisy_signal = signal + noise_amplitude * random.normal(key, signal.shape)\n",
    "\n",
    "# Find the true mean of the noisy signal for use in true_params\n",
    "d_true = jnp.mean(signal) + jnp.mean(noise_amplitude * random.normal(key, signal.shape))\n",
    "\n",
    "# Alter signal with random offset\n",
    "rng, key = random.split(rng)\n",
    "noisy_signal = noisy_signal + random.uniform(key)\n",
    "\n",
    "# bootstrap the mean of the noisy signal\n",
    "keys = random.split(rng, 5001)\n",
    "rng, keys = keys[0], keys[1:]\n",
    "\n",
    "mean_bootstrap_jit = jit(lambda key: mean_bootstrap(key, noisy_signal))\n",
    "\n",
    "# generate 1000 bootstrapped means\n",
    "means = vmap(mean_bootstrap_jit)(keys)\n",
    "\n",
    "# find the mean and standard deviation of the bootstrapped means\n",
    "d_mean = jnp.mean(means)\n",
    "d_variance = jnp.var(means)\n",
    "\n",
    "# Adjust noisy signal for mean \n",
    "noisy_signal = noisy_signal - d_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest 1: Do the linear solver and loss fucntion work in 2D as well?\n",
      "x: [ 40. 200.]\n",
      "[ 0.60587287  0.39676932 -0.07929579]\n",
      "0.010076394\n",
      "\n",
      "\tTest 2: If we perform the 1D optimization on the 2D signal, do we get a reasonable result?\n",
      "optimizer has been called with initial_params: [10.]\n",
      "Initial Setup Complete.\n",
      "Step: 1, Loss: 0.06595175713300705, Tau: [36.393898], Damping: 2.499999936844688e-06\n",
      "Step: 2, Loss: 0.02857314608991146, Tau: [77.41217], Damping: 6.24999984211172e-07\n",
      "Step: 3, Loss: 0.02364525943994522, Tau: [98.09447], Damping: 1.56249996052793e-07\n",
      "Step: 4, Loss: 0.023627430200576782, Tau: [99.470055], Damping: 3.906249901319825e-08\n",
      "Step: 5, Loss: 0.02362741529941559, Tau: [99.5099], Damping: 9.765624753299562e-09\n",
      "LM Time: 1.4696989059448242\n",
      "ADU Time: 0.08426928520202637\n",
      "optimizer has been called with initial_params: [1000.]\n",
      "Initial Setup Complete.\n",
      "Step: 1, Loss: 0.1009390577673912, Tau: [998.5602], Damping: 2.499999936844688e-06\n",
      "Step: 2, Loss: 0.10085462778806686, Tau: [992.73157], Damping: 6.24999984211172e-07\n",
      "Step: 3, Loss: 0.10048823803663254, Tau: [968.22894], Damping: 1.56249996052793e-07\n",
      "Step: 4, Loss: 0.0982956662774086, Tau: [844.1228], Damping: 3.906249901319825e-08\n",
      "Step: 5, Loss: 0.0982956662774086, Tau: [844.1228], Damping: 6.24999984211172e-07\n",
      "Step: 6, Loss: 0.09754537791013718, Tau: [808.80225], Damping: 1.56249996052793e-07\n",
      "Step: 7, Loss: 0.09075457602739334, Tau: [587.4201], Damping: 3.906249901319825e-08\n",
      "Step: 8, Loss: 0.0430477000772953, Tau: [177.22855], Damping: 9.765624753299562e-09\n",
      "Step: 9, Loss: 0.0430477000772953, Tau: [177.22855], Damping: 1.56249996052793e-07\n",
      "Step: 10, Loss: 0.03069809265434742, Tau: [73.22528], Damping: 6.24999984211172e-07\n",
      "Step: 11, Loss: 0.02363869547843933, Tau: [98.38299], Damping: 1.56249996052793e-07\n",
      "Step: 12, Loss: 0.023627422749996185, Tau: [99.48235], Damping: 3.906249901319825e-08\n",
      "Step: 13, Loss: 0.02362741529941559, Tau: [99.50994], Damping: 9.765624753299562e-09\n",
      "LM Time: 0.027473926544189453\n",
      "ADU Time: 0.0\n",
      "\n",
      "\tTest 3: Can we perform a representitive optimization in 2D?\n",
      "optimizer has been called with initial_params: [ 40. 200.]\n",
      "Initial Setup Complete.\n",
      "Step: 1, Loss: 0.010070924647152424, Tau: [ 39.48244 200.57854], Damping: 2.499999936844688e-06\n",
      "Step: 2, Loss: 0.010070924647152424, Tau: [ 39.48244 200.57854], Damping: 3.9999998989515007e-05\n",
      "Step: 3, Loss: 0.010070865042507648, Tau: [ 39.513954 200.56181 ], Damping: 9.999999747378752e-06\n",
      "Step: 4, Loss: 0.01007084734737873, Tau: [ 39.54182 200.52385], Damping: 2.499999936844688e-06\n",
      "Step: 5, Loss: 0.01007084734737873, Tau: [ 39.53682 200.52374], Damping: 9.999999747378752e-06\n",
      "Step: 6, Loss: 0.010070846416056156, Tau: [ 39.53727 200.5182 ], Damping: 2.499999936844688e-06\n",
      "Step: 7, Loss: 0.010070845484733582, Tau: [ 39.537514 200.50633 ], Damping: 6.24999984211172e-07\n",
      "Step: 8, Loss: 0.010070845484733582, Tau: [ 39.533707 200.50516 ], Damping: 2.499999936844688e-06\n",
      "Step: 9, Loss: 0.010070845484733582, Tau: [ 39.535477 200.49146 ], Damping: 9.999999747378752e-06\n",
      "Step: 10, Loss: 0.010070844553411007, Tau: [ 39.531742 200.49239 ], Damping: 2.499999936844688e-06\n",
      "Step: 11, Loss: 0.010070844553411007, Tau: [ 39.532692 200.4824  ], Damping: 9.999999747378752e-06\n",
      "Step: 12, Loss: 0.010070844553411007, Tau: [ 39.530003 200.48271 ], Damping: 3.9999998989515007e-05\n",
      "Step: 13, Loss: 0.010070844553411007, Tau: [ 39.52976 200.48203], Damping: 0.00015999999595806003\n",
      "Step: 14, Loss: 0.010070844553411007, Tau: [ 39.52967 200.48184], Damping: 0.0006399999838322401\n",
      "LM Time: 1.3998618125915527\n",
      "ADU Time: 0.01987290382385254\n",
      "\n",
      "\tTest 4: How does the representitive minimum compare to the true minimum?\n",
      "rmse(true_parms) = 0.010089438408613205\n",
      "rmse(representitive_params) = 0.010070844553411007\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## Testing in 2D! ##\n",
    "####################\n",
    "\n",
    "print(\n",
    "    \"\\tTest 1: Do the linear solver and loss fucntion work in 2D as well?\"\n",
    ")\n",
    "x = params[1::2]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "print(linear_solve(noisy_signal, t, x))\n",
    "print(create_loss_function(t, noisy_signal)(x))\n",
    "\n",
    "print(\n",
    "    \"\\n\\tTest 2: If we perform the 1D optimization on the 2D signal, do we get a reasonable result?\"\n",
    ")\n",
    "lm_optimize = create_lm_optimize(\n",
    "    dim=1,\n",
    "    loss_function=create_loss_function(t, noisy_signal),\n",
    "    max_steps=100,\n",
    "    base_lr=1.0,\n",
    "    initial_damping=1e-5,\n",
    "    damping_scale=4.0,\n",
    "    tol=1e-12,\n",
    ")\n",
    "\n",
    "tau, loss, steps = lm_optimize(jnp.array([10.0]))\n",
    "print(f\"Starting at tau=10.0 - tau: {tau}, loss: {loss}, steps: {steps}\")\n",
    "\n",
    "tau, loss, steps = lm_optimize(jnp.array([1000.0]))\n",
    "print(f\"Starting at tau=1000.0 - tau: {tau}, loss: {loss}, steps: {steps}\")\n",
    "\n",
    "print(\n",
    "    \"\\n\\tTest 3: Can we perform a representitive optimization in 2D?\"\n",
    ")\n",
    "# For those not up to date on the lingo, the representitive minimum is the minimum\n",
    "# that is closest to the true parameters. This can easily be found by starting the \n",
    "# optimization at the true parameters.\n",
    "\n",
    "\n",
    "lm_optimize = create_lm_optimize(\n",
    "    dim=2,\n",
    "    loss_function=create_loss_function(t, noisy_signal),\n",
    "    max_steps=100,\n",
    "    base_lr=1.0,\n",
    "    initial_damping=1e-5,\n",
    "    damping_scale=4.0,\n",
    "    tol=1e-12,\n",
    ")\n",
    "\n",
    "starting_taus = params[1::2]\n",
    "\n",
    "tau, loss, steps = lm_optimize(starting_taus)\n",
    "\n",
    "print(\n",
    "    \"\\n\\tTest 4: How does the representitive minimum compare to the true minimum?\"\n",
    ")\n",
    "true_params = jnp.array([0.6, 40.0, 0.4, 200.0, -d_true])\n",
    "\n",
    "rmse = lambda params: jnp.sqrt(jnp.mean((noisy_signal - multi_decay(params[:-1], t) - params[-1])**2))\n",
    "\n",
    "print(f\"rmse(true_parms) = {rmse(true_params)}\")\n",
    "print(f\"rmse(representitive_params) = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(true_params) = 0.009908615611493587\n",
      "loss(global_min) = 0.009903053753077984\n",
      "predicted true loss: 0.009908507578074932\n",
      "If we assume that the representitive minimum is the global minimum then:\n",
      "loss(true_params) = 0.010593853890895844\n",
      "loss(representitive_params) = 0.010070844553411007\n",
      "predicted true loss: 0.010080093517899513\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## Scaling up the global minimum loss ##\n",
    "########################################\n",
    "\n",
    "# 1D Signal\n",
    "true_loss = 0.009908615611493587\n",
    "global_min_loss = 0.009903053753077984\n",
    "\n",
    "print(f\"loss(true_params) = {true_loss}\")\n",
    "print(f\"loss(global_min) = {global_min_loss}\")\n",
    "\n",
    "scale_up_loss = lambda global_loss, fit_parameter_count, data_points: global_loss/jnp.sqrt(1 - fit_parameter_count/(data_points))\n",
    "\n",
    "print(f\"predicted true loss: {scale_up_loss(global_min_loss, 3, 2726)}\")\n",
    "\n",
    "# 2D Signal\n",
    "true_2 = 0.010593853890895844\n",
    "representitive_2 = 0.010070844553411007\n",
    "\n",
    "print(f\"If we assume that the representitive minimum is the global minimum then:\")\n",
    "print(f\"loss(true_params) = {true_2}\")\n",
    "print(f\"loss(representitive_params) = {representitive_2}\")\n",
    "print(f\"predicted true loss: {scale_up_loss(representitive_2, 5, 2726)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_params: [ 6.0708183e-01  3.9529648e+01  3.9824817e-01  2.0048180e+02\n",
      " -7.9485908e-02]\n",
      "sens shape: (2726, 2726)\n",
      "S trace: 4.999999046325684\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################\n",
    "## Sensitivity Analysis ##\n",
    "##########################\n",
    "\n",
    "# We already have a time array and our fitted parameters\n",
    "# We can use the fitted parameters to create a jacobian function\n",
    "# that will allow us to calculate the sensitivity of the fitted parameters\n",
    "# to the noisy signal.\n",
    "\n",
    "def model_predictions(theta, t):\n",
    "    params, c = theta[:-1], theta[-1]\n",
    "    return multi_decay(params, t) + c\n",
    "\n",
    "predicted_params = jnp.zeros(5)\n",
    "predicted_params = predicted_params.at[1::2].set(tau)\n",
    "predicted_params = predicted_params.at[0::2].set(linear_solve(noisy_signal, t, tau))\n",
    "\n",
    "print(f\"predicted_params: {predicted_params}\")\n",
    "\n",
    "# Compute the Jacobian with respect to theta for a single time point.\n",
    "jac_fn = jacobian(lambda theta, t: model_predictions(theta, t))\n",
    "\n",
    "# Vectorize the Jacobian over all time points.\n",
    "J = vmap(lambda t: jac_fn(predicted_params, t))(t)\n",
    "\n",
    "S = J @ jnp.linalg.inv(J.T @ J) @ J.T\n",
    "\n",
    "print(f\"sens shape: {S.shape}\")\n",
    "\n",
    "print(f\"S trace: {jnp.trace(S)}\")\n",
    "\n",
    "# plot the signal \n",
    "plt.plot(t, noisy_signal, label=\"Noisy Signal\")\n",
    "plt.plot(t, model_predictions(predicted_params, t), label=\"Predicted Signal\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
